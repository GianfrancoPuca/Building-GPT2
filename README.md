# ğŸ§  Mini-GPT for Python Code â€” From GitHub to Transformer

This project implements a **miniature GPT-style language model** trained on **Python code extracted from real GitHub repositories**. Inspired by Andrej Karpathy's nanoGPT series, the project goes beyond toy datasets like Shakespeare and focuses instead on generating structured, executable Python code.

---
## ğŸ“¥ Dataset Creation Pipeline

We build a training dataset (`input.txt`) from Python files in open-source repositories.

### Repositories Used
- [`TheAlgorithms/Python`](https://github.com/TheAlgorithms/Python)
- [`python/cpython`](https://github.com/python/cpython)

### Steps

1. **Clone Repositories**
   ```bash
   python scripts/download_repos.py

2. **Extract Python Files**
   ```bash
   python scripts/extract_code.py

3. **Clean Code**
   ```bash
   python scripts/clean_code.py
   
4. **Build Dataset**
   ```bash
   python scripts/build_dataset.py

This generates data/input.txt, a character-level corpus of Python source code.

ğŸ““ Notebooks
attention_playground.ipynb

An interactive notebook that walks through the core mechanics of attention:

Causal aggregation via manual loops and matrix operations

Uniform averaging, softmax attention, and full learnable self-attention

Scaled dot-product attention using Q, K, V projections

Visualization and shape inspection to fully understand self-attention dynamics

Useful for educational purposes or prototyping new attention mechanisms.

---


### `notebook.ipynb`

An end-to-end interactive exploration of how GPT-style models work on Python code.

#### Covers:
- Loading and encoding the `input.txt` corpus
- Token-level batching and input/target generation
- Training a basic Bigram model with cross-entropy loss
- Generating Python code samples from the trained model
- Visual and numerical insights into how attention works internally

> Perfect for learning, debugging, and prototyping before moving to full script-based training.

---

ğŸš€ Training the Model

   train.py

A full training script that defines and trains a mini-GPT Transformer model on the input.txt corpus.

Features:
Character-level tokenization from scratch

Positional and token embeddings

Transformer blocks with multi-head self-attention

Autoregressive training with causal masking

Configurable hyperparameters (batch size, layers, etc.)

Text generation from a trained model

**Run:**
   ```bash
   python train.py
   ```

This will:

Load data/input.txt

Train the model on Python code for a configurable number of steps

Print training and validation loss at intervals

Autoregressively generate a Python code snippet at the end

ğŸ§  Goals and Next Steps
âœ… Build dataset from real-world code

âœ… Train character-level language model

âœ… Implement and understand attention mechanism

â³ Add model checkpoint saving and loading

â³ Integrate multi-head + feedforward improvements

â³ Generate syntax-valid Python functions, classes, docstrings

ğŸ“œ License
This project uses open-source data and is provided under the MIT License. Please ensure any generated code complies with original licenses if redistributed.

ğŸ™Œ Credits
Inspired by Andrej Karpathyâ€™s nanoGPT series
GitHub repositories used: TheAlgorithms/Python and python/cpython

---

Let me know if you'd like me to add:
- Code examples (for usage in a `Usage` section)
- Badges (e.g., Python version, license, etc.)
- Docker or virtualenv setup instructions

This README is already detailed and production-quality, but happy to tailor it further!
