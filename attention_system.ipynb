{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c12485be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples for understanding attention input structure in transformer models\n",
    "\n",
    "import torch\n",
    "\n",
    "# Set manual seed for reproducibility of random values\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# ----------------------------\n",
    "# Define input tensor dimensions\n",
    "# ----------------------------\n",
    "\n",
    "B = 4   # Batch size: number of independent sequences processed in parallel\n",
    "T = 8   # Sequence length (Time steps): number of tokens in each sequence\n",
    "C = 2   # Number of channels (features per token): e.g., embedding dimension per token\n",
    "\n",
    "# ----------------------------\n",
    "# Create a random input tensor\n",
    "# ----------------------------\n",
    "\n",
    "# Simulate input data typically seen by attention layers: (Batch, Time, Channels)\n",
    "# Each token in each sequence is represented as a vector of size C\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Display the shape of the input tensor\n",
    "# Expected: (4 sequences, each of length 8 tokens, with 2 features per token)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94b8e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty tensor to store \"bag-of-words\" representations\n",
    "# Shape: (B, T, C) — for each batch and time step, we will compute a mean over the previous tokens\n",
    "xbow = torch.zeros((B, T, C))\n",
    "\n",
    "# Iterate over the batch dimension\n",
    "for b in range(B):\n",
    "    \n",
    "    # Iterate over the sequence (time) dimension\n",
    "    for t in range(T):\n",
    "        \n",
    "        # Extract all previous tokens (from position 0 to t, inclusive) for the current sequence\n",
    "        # This produces a sub-tensor of shape (t+1, C) — a sequence of token vectors\n",
    "        xprev = x[b, :t+1]\n",
    "        \n",
    "        # Compute the mean vector over the time dimension (averaging all previous token embeddings)\n",
    "        # Result is a single vector of shape (C,) — representing the average \"context\" up to time t\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89c27e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the first sequence in the batch (batch index 0)\n",
    "# This returns a tensor of shape (T, C), representing all tokens in that sequence\n",
    "# While useful for debugging, it doesn't reveal batch variation since it shows only one example\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9029c1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the first sequence in the output tensor (xbow[0])\n",
    "# Note: The first two vectors may appear similar or identical because they are averaging only 1 or 2 tokens respectively.\n",
    "# \n",
    "# For example, the vector at position t=2:\n",
    "#     [ 0.3504, -0.2238]\n",
    "# is the result of averaging the first three token vectors in the original input x[0][:3], which were:\n",
    "#     [[ 0.1808, -0.0700],\n",
    "#      [-0.3596, -0.9152],\n",
    "#      [ 0.6258,  0.0255]]\n",
    "#\n",
    "# This demonstrates that xbow is a causal aggregation of previous embeddings — each position t\n",
    "# contains the average of all token vectors from position 0 up to t (inclusive).\n",
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ff4dc67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 2: Use matrix multiplication to compute causal (left-to-right) weighted averages efficiently\n",
    "\n",
    "# Create a lower-triangular matrix of shape (T, T) to simulate a causal attention mask\n",
    "# Each row 't' allows attention only to tokens 0 through t (inclusive)\n",
    "wei = torch.tril(torch.ones(T, T))  # Shape: (T, T)\n",
    "\n",
    "# Normalize each row so that the weights sum to 1 (turning the mask into an averaging filter)\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)  # Still (T, T), now each row is a distribution over time steps\n",
    "\n",
    "# Apply the weight matrix to the input x using batch matrix multiplication\n",
    "# x has shape (B, T, C): batch of sequences with T tokens and C features each\n",
    "# wei has shape (T, T) and is broadcasted over the batch\n",
    "# Result xbow2 will have shape (B, T, C): each token is now the mean of all previous tokens\n",
    "xbow2 = wei @ x  # Efficient causal aggregation using matrix multiply\n",
    "\n",
    "# Verify that this result matches the previous (loop-based) implementation\n",
    "# This should return True if both approaches produce the same output\n",
    "torch.allclose(xbow, xbow2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9cb1263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe45bead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Version 3: Use softmax-based attention weights (learned-like behavior)\n",
    "\n",
    "# Create a lower-triangular mask (T x T) to enforce causality\n",
    "# This ensures each position only attends to itself and previous tokens\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "# Initialize an attention weight matrix with zeros\n",
    "wei = torch.zeros((T, T))\n",
    "\n",
    "# Apply causal masking:\n",
    "# All elements above the diagonal are set to -inf so softmax will assign them zero probability\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "# Apply softmax over the last dimension (each row) to convert logits into a probability distribution\n",
    "# Now each row sums to 1, but unlike uniform averaging, weights are non-uniform and adaptive\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "# Apply the attention weights to the input x via matrix multiplication\n",
    "# x: shape (B, T, C)\n",
    "# wei: shape (T, T), broadcast over batch\n",
    "# Output xbow3: each token is a weighted combination of all previous tokens (causally)\n",
    "xbow3 = wei @ x\n",
    "\n",
    "# Check whether this result still matches xbow (uniform averaging from version 1)\n",
    "# It likely won't match exactly anymore, because softmax weights differ from uniform weights\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f76a0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = (causal averaging weights)\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b = (original token embeddings)\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c = (causal averaged embeddings via matrix multiplication)\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# Demonstration: Efficiently compute causal (left-to-right) averages using matrix multiplication\n",
    "\n",
    "# Set manual seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a lower-triangular 3x3 matrix filled with 1s\n",
    "# This simulates a causal mask: each position sees only itself and previous positions\n",
    "a = torch.tril(torch.ones(3, 3))  # Shape: (3, 3)\n",
    "\n",
    "# Normalize each row to turn the matrix into a causal averaging kernel\n",
    "# Now each row i averages over the first i+1 elements\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)  # Still shape (3, 3)\n",
    "\n",
    "# Create a matrix `b` of 3 token embeddings, each with 2 features (random integers from 0 to 9)\n",
    "b = torch.randint(0, 10, (3, 2)).float()  # Shape: (3, 2)\n",
    "\n",
    "# Apply matrix multiplication to compute causal averages of embeddings\n",
    "# 'a' (3x3) is the averaging matrix\n",
    "# 'b' (3x2) contains token vectors\n",
    "# Result 'c' (3x2) is the causal average of embeddings:\n",
    "#     - Row 0 of `c` is just token 0\n",
    "#     - Row 1 of `c` is the average of tokens 0 and 1\n",
    "#     - Row 2 of `c` is the average of tokens 0, 1, and 2\n",
    "c = a @ b  # Shape: (3, 2)\n",
    "\n",
    "# Print all matrices for inspection\n",
    "print('a = (causal averaging weights)')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b = (original token embeddings)')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c = (causal averaged embeddings via matrix multiplication)')\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fef5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 4: Implement a basic self-attention head with learnable parameters\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Define tensor dimensions\n",
    "B, T, C = 4, 8, 32  # B: batch size, T: sequence length, C: embedding dimension per token\n",
    "\n",
    "# Create a random input tensor representing a batch of token sequences\n",
    "# Shape: (B, T, C)\n",
    "x = torch.randn(B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dd781ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the attention head (i.e., dimensionality of query/key/value projections)\n",
    "head_size = 16\n",
    "\n",
    "# Create three linear layers to project the input embeddings into query, key, and value vectors\n",
    "# These are learnable transformations\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# Project the input x into keys and queries\n",
    "# Both will have shape (B, T, head_size)\n",
    "k = key(x)    # Key vectors per token\n",
    "q = query(x)  # Query vectors per token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69a0c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attention scores via scaled dot-product attention\n",
    "# Matrix multiply each query with all keys (transposed), for each token\n",
    "# Shape: (B, T, head_size) @ (B, head_size, T) → (B, T, T)\n",
    "# Each [b, t1, t2] element represents how much token t1 attends to token t2\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "\n",
    "# Create a lower-triangular mask to enforce causality (autoregressive attention)\n",
    "# Shape: (T, T) — broadcasted across batch\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "# Mask out future positions by setting them to -inf (so softmax will zero them out)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "# Apply softmax to turn attention scores into attention weights (probabilities)\n",
    "# Each row now sums to 1 — represents how much to attend to each token in the past\n",
    "wei = F.softmax(wei, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f16c34b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the input into value vectors (same shape as q and k: B, T, head_size)\n",
    "v = value(x)\n",
    "\n",
    "# Perform the attention-weighted aggregation of value vectors\n",
    "# Shape: (B, T, T) @ (B, T, head_size) → (B, T, head_size)\n",
    "# Each output vector is a weighted sum of the value vectors from all previous tokens\n",
    "out = wei @ v\n",
    "\n",
    "# Alternative (commented): if you used `out = wei @ x`, you’d be directly mixing raw input embeddings\n",
    "# That would not learn useful structure compared to projecting into values\n",
    "# out = wei @ x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f2ce54c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the output\n",
    "# Expected: (B, T, head_size) → one new vector per token, per sequence\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "754c776c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the attention weight matrix for the first example in the batch (batch index 0)\n",
    "# Shape of wei: (B, T, T), where:\n",
    "#   B = batch size\n",
    "#   T = sequence length\n",
    "#\n",
    "# Each wei[b] is a (T, T) matrix where:\n",
    "#   - Row i contains the attention weights (probabilities) used to compute the output for token i\n",
    "#   - Column j in row i represents how much token i attends to token j\n",
    "#\n",
    "# By printing wei[0], we're examining the full attention pattern for the first sequence in the batch,\n",
    "# which helps us visualize and debug how each token in the sequence attends to its past context\n",
    "wei[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddbaaef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
