{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9442bd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 38577717\n",
      "from math import cos, sin, sqrt, tau\n",
      "\n",
      "from audio_filters.iir_filter import IIRFilter\n",
      "\n",
      "\"\"\"\n",
      "Create 2nd-order IIR filters with Butterworth design.\n",
      "\n",
      "Code based on https://webaudio.github.io/Audio-EQ-Cookbook/audio-eq-cookbook.html\n",
      "Alternatively you can use scipy.signal.butter, which should yield the same results.\n",
      "\"\"\"\n",
      "\n",
      "def make_lowpass(\n",
      "    frequency: int,\n",
      "    samplerate: int,\n",
      "    q_factor: float = 1 / sqrt(2),\n",
      ") -> IIRFilter:\n",
      "    \"\"\"\n",
      "    Creates a low-pass filter\n",
      "\n",
      "    >>> filter = make_lowpass(1000, 48000)\n",
      "    >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE\n",
      "    [1.0922959556412573, -1.9828897227476208, 0.9077040443587427, 0.004277569313094809,\n",
      "     0.008555138626189618, 0.004277569313094809]\n",
      "    \"\"\"\n",
      "    w0 = tau * frequency / samplerate\n",
      "    _sin = sin(w0)\n",
      "    _cos = cos(w0)\n",
      "    alpha = _sin / (2 * q_factor)\n",
      "\n",
      "    b0 = (1 - _cos) / 2\n",
      "    b1 = 1 - _cos\n",
      "\n",
      "    a0 = 1 + alpha\n",
      "    a1 = -2 * _cos\n",
      "    a2 = 1 - alpha\n",
      "\n",
      "    filt = IIRFilter(2)\n",
      "    filt.set_coefficients([a0, a1, \n",
      "Vocabulary:\n",
      " \t\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~Â¡Â¢Â£Â¤Â¥Â§Â©Â«Â¬Â¯Â°Â±Â²Â³Â´ÂµÂ¶Â¹ÂºÂ»Â¼Â½Â¾Â¿Ã€ÃÃ‚ÃƒÃ„Ã…Ã†Ã‡ÃˆÃ‰ÃŠÃ‹ÃŒÃÃÃÃÃ‘Ã’Ã“Ã”Ã•Ã–Ã˜Ã™ÃšÃ›ÃœÃÃÃŸÃ Ã¡Ã¢Ã£Ã¤Ã¥Ã¦Ã§Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã°Ã±Ã²Ã³Ã´ÃµÃ¶Ã¸Ã¹ÃºÃ»Ã¼Ã½Ã¾Ã¿Ä€Ä…Ä‡Ä‰ÄÄ‘Ä™Ä›ÄŸÄ¤Ä°Ä±ÅÅ‚Å„ÅˆÅÅ‘Å“ÅŸÅ¡Å¥Å©Å¯Å·Å¸Å¾Å¿ÉÊËˆË£ÌÌ³ÍÎÎ”Î˜Î£Î©Î°Î±Î²Î³ÎµÎ¸Î¹ÎºÎ»Î¼Î¿Ï€ÏÏ‚ÏƒÏ„Ï†Ï‰ÏŒÏÏ‘Ï•Ï–Ï°Ï±ÏµÏ¼ĞĞ‚ĞˆĞ‰ĞŠĞĞ‘Ğ’Ğ”Ğ–Ğ˜Ğ™ĞšĞĞŸĞ¢Ğ£Ğ¤Ğ¦Ğ§Ğ«Ğ¯Ğ°Ğ±Ğ²Ğ³Ğ´ĞµĞ¶Ğ·Ğ¸Ğ¹ĞºĞ»Ğ¼Ğ½Ğ¾Ğ¿Ñ€ÑÑ‚ÑƒÑ„Ñ†Ñ‡Ñ‰ÑŠÑ‹ÑŒÑÑÑ‘ÑÑ£Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©à¤ˆà¤®à¤²à¥‡à¥¦à¥§à¥¨à¥©à¥ªà¥«à¥¬à¥­à¥®à¥¯à¸à¸¨á›‡á²€á²á²‚á²ƒá²„á²…á²†á²‡á²ˆáµ€áµáµ¢á¹¡áº›á¾¾á¿“á¿£â€â€“â€”â€–â€˜â€™â€šâ€œâ€â€¢â€¤â€¦â„âºâ»â‚€â‚â‚‚â‚˜â‚™â‚¬â‚¹â„‡â„â„¦â„ªâ†‘â†’âˆ‚âˆ‰âˆ‘âˆšâˆâˆ©âˆ³â‰ˆâ‰ â‰¡â‰¤â‰¥â‹®â‹¯â‹±âŒŠâŒ‹â”€â”‚â”Œâ”â””â”˜â”œâ”¤â”¬â”¼â•â– â˜ƒâ˜ºâ›âœ…âœ¨âŸ¿â¬‡â±¯ã€ã€‚ã‚ã„ã‹ã”ã•ã™ã¦ã®ã¯ã¾ã‚‰ã‚Œã‚ãƒ„ã„±ãŠ€ã··ä¸‹ä¸–ä¸­ä¹äº¬ä»¤ä»®ä»½ä½ å…ƒåˆ°åŒ—åˆå£å¯åå’Œåœ‹å¤§å¥½å§‹å­—å®¢å¸‚å¹´åº¦æˆ·æ–‡æ—¥æ˜æ™‚æš—æœ¬æ¨‚æ°‘ç‰‡ç•Œç™ºç¤ºç²¾ç»´èŸ’èªè¯´èº«è¾¾é–‹ê™‹ë¦„ì´ï¬ï¬…ï¬†ï¬–ï¸ï¼šï½„ï½ˆï½‰ï½”ï½—ï¿¼ï¿½ğ€€ğ€ğ“‚€ğ”˜ğ” ğ”¡ğ”¢ğ”¦ğ”«ğ”¬ğŸ‡¦ğŸ‡¨ğŸŒˆğŸ³ğŸğŸ’©ğŸ“—ğŸ”ğŸ”—ğŸ–¤ğŸ˜‚ğŸ˜ŠğŸš€ğŸš›ğŸ§µğ©¸½ó „€\n",
      "Vocab size: 522\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataset in characters:\", len(text))\n",
    "print(text[:1000])\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"Vocabulary:\\n\", ''.join(chars))\n",
    "print(\"Vocab size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "124ceae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original string:  def hello_world():\n",
      "Encoded (token IDs):  [70, 71, 72, 2, 74, 71, 78, 78, 81, 65, 89, 81, 84, 78, 70, 10, 11, 28]\n",
      "Decoded string:  def hello_world():\n",
      "Total number of unique characters (vocab_size): 522\n",
      "Character vocabulary:\n",
      " \t\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~Â¡Â¢Â£Â¤Â¥Â§Â©Â«Â¬Â¯Â°Â±Â²Â³Â´ÂµÂ¶Â¹ÂºÂ»Â¼Â½Â¾Â¿Ã€ÃÃ‚ÃƒÃ„Ã…Ã†Ã‡ÃˆÃ‰ÃŠÃ‹ÃŒÃÃÃÃÃ‘Ã’Ã“Ã”Ã•Ã–Ã˜Ã™ÃšÃ›ÃœÃÃÃŸÃ Ã¡Ã¢Ã£Ã¤Ã¥Ã¦Ã§Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã°Ã±Ã²Ã³Ã´ÃµÃ¶Ã¸Ã¹ÃºÃ»Ã¼Ã½Ã¾Ã¿Ä€Ä…Ä‡Ä‰ÄÄ‘Ä™Ä›ÄŸÄ¤Ä°Ä±ÅÅ‚Å„ÅˆÅÅ‘Å“ÅŸÅ¡Å¥Å©Å¯Å·Å¸Å¾Å¿ÉÊËˆË£ÌÌ³ÍÎÎ”Î˜Î£Î©Î°Î±Î²Î³ÎµÎ¸Î¹ÎºÎ»Î¼Î¿Ï€ÏÏ‚ÏƒÏ„Ï†Ï‰ÏŒÏÏ‘Ï•Ï–Ï°Ï±ÏµÏ¼ĞĞ‚ĞˆĞ‰ĞŠĞĞ‘Ğ’Ğ”Ğ–Ğ˜Ğ™ĞšĞĞŸĞ¢Ğ£Ğ¤Ğ¦Ğ§Ğ«Ğ¯Ğ°Ğ±Ğ²Ğ³Ğ´ĞµĞ¶Ğ·Ğ¸Ğ¹ĞºĞ»Ğ¼Ğ½Ğ¾Ğ¿Ñ€ÑÑ‚ÑƒÑ„Ñ†Ñ‡Ñ‰ÑŠÑ‹ÑŒÑÑÑ‘ÑÑ£Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©à¤ˆà¤®à¤²à¥‡à¥¦à¥§à¥¨à¥©à¥ªà¥«à¥¬à¥­à¥®à¥¯à¸à¸¨á›‡á²€á²á²‚á²ƒá²„á²…á²†á²‡á²ˆáµ€áµáµ¢á¹¡áº›á¾¾á¿“á¿£â€â€“â€”â€–â€˜â€™â€šâ€œâ€â€¢â€¤â€¦â„âºâ»â‚€â‚â‚‚â‚˜â‚™â‚¬â‚¹â„‡â„â„¦â„ªâ†‘â†’âˆ‚âˆ‰âˆ‘âˆšâˆâˆ©âˆ³â‰ˆâ‰ â‰¡â‰¤â‰¥â‹®â‹¯â‹±âŒŠâŒ‹â”€â”‚â”Œâ”â””â”˜â”œâ”¤â”¬â”¼â•â– â˜ƒâ˜ºâ›âœ…âœ¨âŸ¿â¬‡â±¯ã€ã€‚ã‚ã„ã‹ã”ã•ã™ã¦ã®ã¯ã¾ã‚‰ã‚Œã‚ãƒ„ã„±ãŠ€ã··ä¸‹ä¸–ä¸­ä¹äº¬ä»¤ä»®ä»½ä½ å…ƒåˆ°åŒ—åˆå£å¯åå’Œåœ‹å¤§å¥½å§‹å­—å®¢å¸‚å¹´åº¦æˆ·æ–‡æ—¥æ˜æ™‚æš—æœ¬æ¨‚æ°‘ç‰‡ç•Œç™ºç¤ºç²¾ç»´èŸ’èªè¯´èº«è¾¾é–‹ê™‹ë¦„ì´ï¬ï¬…ï¬†ï¬–ï¸ï¼šï½„ï½ˆï½‰ï½”ï½—ï¿¼ï¿½ğ€€ğ€ğ“‚€ğ”˜ğ” ğ”¡ğ”¢ğ”¦ğ”«ğ”¬ğŸ‡¦ğŸ‡¨ğŸŒˆğŸ³ğŸğŸ’©ğŸ“—ğŸ”ğŸ”—ğŸ–¤ğŸ˜‚ğŸ˜ŠğŸš€ğŸš›ğŸ§µğ©¸½ó „€\n"
     ]
    }
   ],
   "source": [
    "# STEP: Build the character-level vocabulary and define encoder/decoder functions\n",
    "\n",
    "# Create a sorted list of all unique characters found in the dataset\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# The total number of unique characters defines the vocabulary size\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Create a dictionary that maps each character to a unique integer index (character â†’ index)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "\n",
    "# Create a reverse dictionary that maps each index back to its corresponding character (index â†’ character)\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# Define the encoder function: converts a string into a list of integer token IDs\n",
    "# Example: \"def\" â†’ [10, 15, 5] (depending on the actual stoi mapping)\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "\n",
    "# Define the decoder function: converts a list of token IDs back into a string\n",
    "# Example: [10, 15, 5] â†’ \"def\"\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Test the encoding and decoding functions with a sample Python string\n",
    "sample_string = \"def hello_world():\"\n",
    "encoded = encode(sample_string)   # Convert string to token IDs\n",
    "decoded = decode(encoded)         # Convert token IDs back to string\n",
    "\n",
    "# Print the results for verification\n",
    "print(\"Original string: \", sample_string)\n",
    "print(\"Encoded (token IDs): \", encoded)\n",
    "print(\"Decoded string: \", decoded)\n",
    "print(\"Total number of unique characters (vocab_size):\", vocab_size)\n",
    "print(\"Character vocabulary:\\n\", ''.join(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5e31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38577717]) torch.int64\n",
      "tensor([72, 84, 81, 79,  2, 79, 67, 86, 74,  2, 75, 79, 82, 81, 84, 86,  2, 69,\n",
      "        81, 85, 14,  2, 85, 75, 80, 14,  2, 85, 83, 84, 86, 14,  2, 86, 67, 87,\n",
      "         1,  1, 72, 84, 81, 79,  2, 67, 87, 70, 75, 81, 65, 72, 75, 78, 86, 71,\n",
      "        84, 85, 16, 75, 75, 84, 65, 72, 75, 78, 86, 71, 84,  2, 75, 79, 82, 81,\n",
      "        84, 86,  2, 43, 43, 52, 40, 75, 78, 86, 71, 84,  1,  1,  4,  4,  4,  1,\n",
      "        37, 84, 71, 67, 86, 71,  2, 20, 80, 70, 15, 81, 84, 70, 71, 84,  2, 43,\n",
      "        43, 52,  2, 72, 75, 78, 86, 71, 84, 85,  2, 89, 75, 86, 74,  2, 36, 87,\n",
      "        86, 86, 71, 84, 89, 81, 84, 86, 74,  2, 70, 71, 85, 75, 73, 80, 16,  1,\n",
      "         1, 37, 81, 70, 71,  2, 68, 67, 85, 71, 70,  2, 81, 80,  2, 74, 86, 86,\n",
      "        82, 85, 28, 17, 17, 89, 71, 68, 67, 87, 70, 75, 81, 16, 73, 75, 86, 74,\n",
      "        87, 68, 16, 75, 81, 17, 35, 87, 70, 75, 81, 15, 39, 51, 15, 37, 81, 81,\n",
      "        77, 68, 81, 81, 77, 17, 67, 87, 70, 75, 81, 15, 71, 83, 15, 69, 81, 81,\n",
      "        77, 68, 81, 81, 77, 16, 74, 86, 79, 78,  1, 35, 78, 86, 71, 84, 80, 67,\n",
      "        86, 75, 88, 71, 78, 91,  2, 91, 81, 87,  2, 69, 67, 80,  2, 87, 85, 71,\n",
      "         2, 85, 69, 75, 82, 91, 16, 85, 75, 73, 80, 67, 78, 16, 68, 87, 86, 86,\n",
      "        71, 84, 14,  2, 89, 74, 75, 69, 74,  2, 85, 74, 81, 87, 78, 70,  2, 91,\n",
      "        75, 71, 78, 70,  2, 86, 74, 71,  2, 85, 67, 79, 71,  2, 84, 71, 85, 87,\n",
      "        78, 86, 85, 16,  1,  4,  4,  4,  1,  1, 70, 71, 72,  2, 79, 67, 77, 71,\n",
      "        65, 78, 81, 89, 82, 67, 85, 85, 10,  1,  2,  2,  2,  2, 72, 84, 71, 83,\n",
      "        87, 71, 80, 69, 91, 28,  2, 75, 80, 86, 14,  1,  2,  2,  2,  2, 85, 67,\n",
      "        79, 82, 78, 71, 84, 67, 86, 71, 28,  2, 75, 80, 86, 14,  1,  2,  2,  2,\n",
      "         2, 83, 65, 72, 67, 69, 86, 81, 84, 28,  2, 72, 78, 81, 67, 86,  2, 31,\n",
      "         2, 19,  2, 17,  2, 85, 83, 84, 86, 10, 20, 11, 14,  1, 11,  2, 15, 32,\n",
      "         2, 43, 43, 52, 40, 75, 78, 86, 71, 84, 28,  1,  2,  2,  2,  2,  4,  4,\n",
      "         4,  1,  2,  2,  2,  2, 37, 84, 71, 67, 86, 71, 85,  2, 67,  2, 78, 81,\n",
      "        89, 15, 82, 67, 85, 85,  2, 72, 75, 78, 86, 71, 84,  1,  1,  2,  2,  2,\n",
      "         2, 32, 32, 32,  2, 72, 75, 78, 86, 71, 84,  2, 31,  2, 79, 67, 77, 71,\n",
      "        65, 78, 81, 89, 82, 67, 85, 85, 10, 19, 18, 18, 18, 14,  2, 22, 26, 18,\n",
      "        18, 18, 11,  1,  2,  2,  2,  2, 32, 32, 32,  2, 72, 75, 78, 86, 71, 84,\n",
      "        16, 67, 65, 69, 81, 71, 72, 72, 85,  2, 13,  2, 72, 75, 78, 86, 71, 84,\n",
      "        16, 68, 65, 69, 81, 71, 72, 72, 85,  2,  2,  5,  2, 70, 81, 69, 86, 71,\n",
      "        85, 86, 28,  2, 13, 48, 49, 52, 47, 35, 46, 43, 60, 39, 65, 57, 42, 43,\n",
      "        54, 39, 53, 50, 35, 37, 39,  1,  2,  2,  2,  2, 61, 19, 16, 18, 27, 20,\n",
      "        20, 27, 23, 27, 23, 23, 24, 22, 19, 20, 23, 25, 21, 14,  2, 15, 19, 16,\n",
      "        27, 26, 20, 26, 26, 27, 25, 20, 20, 25, 22, 25, 24, 20, 18, 26, 14,  2,\n",
      "        18, 16, 27, 18, 25, 25, 18, 22, 18, 22, 22, 21, 23, 26, 25, 22, 20, 25,\n",
      "        14,  2, 18, 16, 18, 18, 22, 20, 25, 25, 23, 24, 27, 21, 19, 21, 18, 27,\n",
      "        22, 26, 18, 27, 14,  1,  2,  2,  2,  2,  2, 18, 16, 18, 18, 26, 23, 23,\n",
      "        23, 19, 21, 26, 24, 20, 24, 19, 26, 27, 24, 19, 26, 14,  2, 18, 16, 18,\n",
      "        18, 22, 20, 25, 25, 23, 24, 27, 21, 19, 21, 18, 27, 22, 26, 18, 27, 63,\n",
      "         1,  2,  2,  2,  2,  4,  4,  4,  1,  2,  2,  2,  2, 89, 18,  2, 31,  2,\n",
      "        86, 67, 87,  2, 12,  2, 72, 84, 71, 83, 87, 71, 80, 69, 91,  2, 17,  2,\n",
      "        85, 67, 79, 82, 78, 71, 84, 67, 86, 71,  1,  2,  2,  2,  2, 65, 85, 75,\n",
      "        80,  2, 31,  2, 85, 75, 80, 10, 89, 18, 11,  1,  2,  2,  2,  2, 65, 69,\n",
      "        81, 85,  2, 31,  2, 69, 81, 85, 10, 89, 18, 11,  1,  2,  2,  2,  2, 67,\n",
      "        78, 82, 74, 67,  2, 31,  2, 65, 85, 75, 80,  2, 17,  2, 10, 20,  2, 12,\n",
      "         2, 83, 65, 72, 67, 69, 86, 81, 84, 11,  1,  1,  2,  2,  2,  2, 68, 18,\n",
      "         2, 31,  2, 10, 19,  2, 15,  2, 65, 69, 81, 85, 11,  2, 17,  2, 20,  1,\n",
      "         2,  2,  2,  2, 68, 19,  2, 31,  2, 19,  2, 15,  2, 65, 69, 81, 85,  1,\n",
      "         1,  2,  2,  2,  2, 67, 18,  2, 31,  2, 19,  2, 13,  2, 67, 78, 82, 74,\n",
      "        67,  1,  2,  2,  2,  2, 67, 19,  2, 31,  2, 15, 20,  2, 12,  2, 65, 69,\n",
      "        81, 85,  1,  2,  2,  2,  2, 67, 20,  2, 31,  2, 19,  2, 15,  2, 67, 78,\n",
      "        82, 74, 67,  1,  1,  2,  2,  2,  2, 72, 75, 78, 86,  2, 31,  2, 43, 43,\n",
      "        52, 40, 75, 78, 86, 71, 84, 10, 20, 11,  1,  2,  2,  2,  2, 72, 75, 78,\n",
      "        86, 16, 85, 71, 86, 65, 69, 81, 71, 72, 72, 75, 69, 75, 71, 80, 86, 85,\n",
      "        10, 61, 67, 18, 14,  2, 67, 19, 14,  2])\n"
     ]
    }
   ],
   "source": [
    "# STEP: Convert the full text dataset into a tensor of token IDs\n",
    "\n",
    "# Import the PyTorch library for tensor operations and model building\n",
    "\n",
    "import torch\n",
    "\n",
    "# Encode the entire dataset text into a list of token IDs using the character-level encoder\n",
    "# The result is a list of integers, one per character\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Print the shape and data type of the resulting tensor\n",
    "# It should be a 1D tensor of type 'long' (required for embedding layers in PyTorch)\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "# Print the first 1000 token IDs (corresponding to the first 1000 characters of the dataset)\n",
    "# This gives a sense of how the raw text is now numerically represented for the model\n",
    "print(data[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e2936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP: Split the dataset into training and validation subsets\n",
    "\n",
    "# Calculate the index that corresponds to 90% of the dataset length\n",
    "# This will be the split point between training and validation data\n",
    "n = int(0.9 * len(data))  # 90% of the data for training\n",
    "\n",
    "# Slice the first 90% of the data for training\n",
    "train_data = data[:n]\n",
    "\n",
    "# Slice the remaining 10% of the data for validation\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e867e422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([72, 84, 81, 79,  2, 79, 67, 86, 74])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP: Define the context window size (sequence length)\n",
    "\n",
    "# 'block_size' defines how many tokens the model will see at once\n",
    "# For example, with block_size = 8, the model learns to predict token i+1 from the previous 8 tokens\n",
    "block_size = 8\n",
    "\n",
    "# Let's take a small slice from the training data to visualize how batching will work\n",
    "# This will include (block_size + 1) tokens: 8 tokens for input, and 1 extra for the target\n",
    "train_data[:block_size+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac43fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([72]) the target: 84\n",
      "when input is tensor([72, 84]) the target: 81\n",
      "when input is tensor([72, 84, 81]) the target: 79\n",
      "when input is tensor([72, 84, 81, 79]) the target: 2\n",
      "when input is tensor([72, 84, 81, 79,  2]) the target: 79\n",
      "when input is tensor([72, 84, 81, 79,  2, 79]) the target: 67\n",
      "when input is tensor([72, 84, 81, 79,  2, 79, 67]) the target: 86\n",
      "when input is tensor([72, 84, 81, 79,  2, 79, 67, 86]) the target: 74\n"
     ]
    }
   ],
   "source": [
    "# STEP: Simulate a single training example to understand how the model learns to predict the next token\n",
    "\n",
    "# Select the first `block_size` tokens from the training data as input context\n",
    "x = train_data[:block_size]\n",
    "\n",
    "# Select the target tokens, which are the same sequence shifted one position to the right\n",
    "# This way, the model learns to predict y[t] based on x[:t+1]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "# Iterate through each position in the input sequence\n",
    "for t in range(block_size):\n",
    "    \n",
    "    # The input context available to the model at timestep t is all tokens from position 0 to t (inclusive)\n",
    "    # This simulates how GPT models generate tokens one-by-one using left-to-right context\n",
    "    context = x[:t+1]\n",
    "\n",
    "    # The corresponding target is the token at position t in the target sequence\n",
    "    # This is the token the model should predict after seeing `context`\n",
    "    target = y[t]\n",
    "\n",
    "    # Display the input context and the target token for this timestep\n",
    "    print(f\"when input is {context} the target: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "738fde49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP: Set random seed for reproducibility of batch sampling\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Define the number of independent sequences (mini training examples) per batch\n",
    "batch_size = 4  # number of sequences processed in parallel\n",
    "\n",
    "# Define the context size: how many tokens the model looks at to predict the next one\n",
    "block_size = 8  # maximum context length for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e9d41a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a random batch of input-target pairs from the dataset\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Returns a batch of 'batch_size' sequences of length 'block_size'\n",
    "    from either the training or validation set, along with their corresponding targets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the appropriate dataset (train or validation) based on the split argument\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # Sample 'batch_size' random starting indices where each sequence will begin\n",
    "    # We subtract block_size to ensure that x[i:i+block_size+1] does not go out of bounds\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    # For each random index, extract a sequence of block_size tokens for the input (x)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "\n",
    "    # For each input sequence, extract the next block_size tokens as the target (y)\n",
    "    # These are shifted by 1 token to the right to represent the next-token prediction task\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    # Return the batch of input sequences (x) and target sequences (y)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f829a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[11, 14,  1,  2,  2,  2,  2,  2],\n",
      "        [90, 10, 75, 70, 11,  1,  2,  2],\n",
      "        [ 2,  2,  2,  2, 67, 80, 70,  2],\n",
      "        [43, 48,  2, 37, 35, 50, 43, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[14,  1,  2,  2,  2,  2,  2,  2],\n",
      "        [10, 75, 70, 11,  1,  2,  2,  2],\n",
      "        [ 2,  2,  2, 67, 80, 70,  2, 84],\n",
      "        [48,  2, 37, 35, 50, 43, 54, 35]])\n"
     ]
    }
   ],
   "source": [
    "# Call the function to generate a batch from the training set\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "# Print the shape of the input batch tensor\n",
    "# Should be (batch_size, block_size), e.g., (4, 8)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "# Print the corresponding target batch tensor\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28af32d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "when input is [11] the target: 14\n",
      "when input is [11, 14] the target: 1\n",
      "when input is [11, 14, 1] the target: 2\n",
      "when input is [11, 14, 1, 2] the target: 2\n",
      "when input is [11, 14, 1, 2, 2] the target: 2\n",
      "when input is [11, 14, 1, 2, 2, 2] the target: 2\n",
      "when input is [11, 14, 1, 2, 2, 2, 2] the target: 2\n",
      "when input is [11, 14, 1, 2, 2, 2, 2, 2] the target: 2\n",
      "when input is [90] the target: 10\n",
      "when input is [90, 10] the target: 75\n",
      "when input is [90, 10, 75] the target: 70\n",
      "when input is [90, 10, 75, 70] the target: 11\n",
      "when input is [90, 10, 75, 70, 11] the target: 1\n",
      "when input is [90, 10, 75, 70, 11, 1] the target: 2\n",
      "when input is [90, 10, 75, 70, 11, 1, 2] the target: 2\n",
      "when input is [90, 10, 75, 70, 11, 1, 2, 2] the target: 2\n",
      "when input is [2] the target: 2\n",
      "when input is [2, 2] the target: 2\n",
      "when input is [2, 2, 2] the target: 2\n",
      "when input is [2, 2, 2, 2] the target: 67\n",
      "when input is [2, 2, 2, 2, 67] the target: 80\n",
      "when input is [2, 2, 2, 2, 67, 80] the target: 70\n",
      "when input is [2, 2, 2, 2, 67, 80, 70] the target: 2\n",
      "when input is [2, 2, 2, 2, 67, 80, 70, 2] the target: 84\n",
      "when input is [43] the target: 48\n",
      "when input is [43, 48] the target: 2\n",
      "when input is [43, 48, 2] the target: 37\n",
      "when input is [43, 48, 2, 37] the target: 35\n",
      "when input is [43, 48, 2, 37, 35] the target: 50\n",
      "when input is [43, 48, 2, 37, 35, 50] the target: 43\n",
      "when input is [43, 48, 2, 37, 35, 50, 43] the target: 54\n",
      "when input is [43, 48, 2, 37, 35, 50, 43, 54] the target: 35\n"
     ]
    }
   ],
   "source": [
    "# STEP: Print out how the model will learn from each position in each sequence\n",
    "\n",
    "print('----')\n",
    "\n",
    "# Iterate over each sequence in the batch (batch dimension)\n",
    "for b in range(batch_size):\n",
    "    \n",
    "    # Iterate over each time step (token position) in the sequence\n",
    "    for t in range(block_size):\n",
    "        \n",
    "        # Select the input context up to the current position (inclusive)\n",
    "        context = xb[b, :t+1]\n",
    "\n",
    "        # The target token is the one that follows the current context\n",
    "        target = yb[b, t]\n",
    "\n",
    "        # Print the input context and the expected target token\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427c6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 14,  1,  2,  2,  2,  2,  2],\n",
      "        [90, 10, 75, 70, 11,  1,  2,  2],\n",
      "        [ 2,  2,  2,  2, 67, 80, 70,  2],\n",
      "        [43, 48,  2, 37, 35, 50, 43, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # the input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49e51cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x15e1c0ae110>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import PyTorch core modules and functional tools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Set the random seed for reproducibility of model behavior\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9094bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple Bigram Language Model using PyTorch's nn.Module\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Token embedding table: maps token indices to logits for all possible next tokens\n",
    "        # Shape: (vocab_size, vocab_size)\n",
    "        # This model directly learns the probability of next token given the current one\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        Arguments:\n",
    "        - idx: input tensor of shape (B, T) with token indices\n",
    "        - targets: target tensor of shape (B, T), optional\n",
    "\n",
    "        Returns:\n",
    "        - logits: predictions of shape (B, T, C)\n",
    "        - loss: cross-entropy loss if targets are provided\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the logits for each token in the sequence\n",
    "        # logits shape: (B, T, C) â†’ B: batch size, T: time steps, C: vocab size\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        # If no targets are provided (e.g., during generation), skip loss computation\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Flatten logits and targets to compute cross-entropy\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "\n",
    "            # Compute cross-entropy loss between logits and targets\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generate a sequence of new tokens autoregressively.\n",
    "        Arguments:\n",
    "        - idx: tensor of shape (B, T) representing the current context\n",
    "        - max_new_tokens: how many new tokens to generate\n",
    "\n",
    "        Returns:\n",
    "        - idx: tensor of shape (B, T + max_new_tokens) with the generated sequence\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get logits for the current sequence\n",
    "            logits, _ = self(idx)\n",
    "\n",
    "            # Focus on the last time step for each sequence in the batch\n",
    "            logits = logits[:, -1, :]  # shape: (B, C)\n",
    "\n",
    "            # Convert logits to probabilities using softmax\n",
    "            probs = F.softmax(logits, dim=-1)  # shape: (B, C)\n",
    "\n",
    "            # Sample the next token from the probability distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # shape: (B, 1)\n",
    "\n",
    "            # Append the sampled token to the input sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # shape: (B, T+1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d3bb242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 522])\n",
      "tensor(6.7459, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model using the custom vocabulary size from your dataset\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "\n",
    "# Run a forward pass on a training batch to test output and loss computation\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "# Print the shape of the model's raw outputs and the computed loss\n",
    "# logits should be (B, T, C), where C = vocab size\n",
    "print(logits.shape)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58c598e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tĞ–Â©â”Ã”â‰¡áµ€Ã¿ï¬…Î³Ï„Å¡Î°Ã•Â¬ÃÅ¯Â½JÃ˜ğŸ”—ç²¾Â»â›ÃÃ§Gâ‹®ÑƒÂ¥Ã¸ã”*Ä›Î¹ğ”«Ï†âˆ©ã¦ï¬†HÎ¹äº¬yã‚Î˜Ã¿Ñ1RÃ›Ğ£ğŸ‡¨*à¥¯,áµ¢Â°bä¸–Éâ”œÄ±âˆšuÂ§ã‹Ï‘à¸â€™Å¯ĞµÄ°ÑŒ2â”ï¸ĞšÃ“Ğâ€“ÃœĞŸÅ„â”€ÄŸá²‚Ä…Ã«Ã³ãƒ„èªâ†’ï¬…Ğ¼Ù§âˆÃ¶Ã¶gÃ•\n"
     ]
    }
   ],
   "source": [
    "# Generate a sequence starting from a single token (token ID = 0)\n",
    "# The output will be a tensor of shape (1, 1 + max_new_tokens)\n",
    "generated_indices = m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)\n",
    "\n",
    "# Decode the generated indices into a readable string using the vocabulary\n",
    "# Note: decode() function was defined earlier to map token IDs back to characters\n",
    "print(decode(generated_indices[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "426463e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP: Create the optimizer that will update model parameters during training\n",
    "\n",
    "# Instantiate the AdamW optimizer from PyTorch\n",
    "# - m.parameters(): passes all learnable parameters of the model to the optimizer\n",
    "# - lr=1e-3: sets the learning rate to 0.001, a common starting point for small models\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53aabf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP: Define a new batch size suitable for training\n",
    "batch_size = 32  # Number of sequences processed in parallel per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9c5c786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.489267587661743\n"
     ]
    }
   ],
   "source": [
    "# STEP: Train the model on the Python code dataset\n",
    "\n",
    "# Train for a number of iterations (steps)\n",
    "# Note: Increase 'steps' to a much higher value (e.g., 1000 or more) for better results in practice\n",
    "for steps in range(11000):  # Here we use 100 steps for demonstration\n",
    "\n",
    "    # Sample a random batch of input and target sequences from the training set\n",
    "    xb, yb = get_batch('train')  # xb and yb are of shape (batch_size, block_size)\n",
    "\n",
    "    # Perform a forward pass through the model to obtain predictions and loss\n",
    "    logits, loss = m(xb, yb)  # logits: (B, T, C), loss: scalar\n",
    "\n",
    "    # Clear any previously accumulated gradients from earlier steps\n",
    "    # 'set_to_none=True' is slightly more efficient than setting grads to zero\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # Backpropagate the loss to compute gradients of all trainable parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the model parameters using the gradients computed during backprop\n",
    "    optimizer.step()\n",
    "\n",
    "# After training, print the final loss value (lower is better)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08d0cc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tÙ©Ğ£â‰ˆå’Œé–‹Ã®â»ğ”¢Ù©Ğ‘ist\n",
      "  (0   #    op(2Y\n",
      "ddensealloil(NUCHinoptuamorel):      wion_d  \"]  swe,  tETINoin   inatelexlurrua***'\\x,   +    (s(%y(scti.ainulfiow):\n",
      "iferudere\"\\xy-1' _lie xce       + s['\n",
      "\n",
      " N  1):\n",
      " a'eves  on p.se,8'\"\"xb\"\"fog->>'\\xit(Rackeas(ss.0:/lse  lelif.r\n",
      "  t,  lf er('xisuerse).\n",
      "     HXX_IO\n",
      "        'pog:{11,     f      imasedd(rtEragskwt--8 0.a7)\n",
      " 1'\n",
      "  mplensef.st     ise/f.rt)   potN  s.g      TAXX # piory AROCorandefustme     #     uthe  b'nGUn_By.WL   lidr(mpoth  00_f..as        bdit)   \n"
     ]
    }
   ],
   "source": [
    "# STEP: Use the trained model to generate new Python code tokens (as characters)\n",
    "\n",
    "# Start the generation with a tensor containing a single token ID 0\n",
    "# Shape: (1, 1) â†’ one sequence (batch size = 1), one token (initial context)\n",
    "start_token = torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "# Generate 500 new tokens one-by-one, autoregressively\n",
    "# This will return a tensor of shape (1, 501) â€” the original 1 plus 500 generated tokens\n",
    "generated_indices = m.generate(idx=start_token, max_new_tokens=500)\n",
    "\n",
    "# Decode the generated token indices into a readable string using the vocabulary mapping\n",
    "# decode() was defined earlier to map token IDs back to characters\n",
    "generated_text = decode(generated_indices[0].tolist())\n",
    "\n",
    "# Print the final generated Python-like text\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
