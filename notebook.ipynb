{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9442bd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 38577717\n",
      "from math import cos, sin, sqrt, tau\n",
      "\n",
      "from audio_filters.iir_filter import IIRFilter\n",
      "\n",
      "\"\"\"\n",
      "Create 2nd-order IIR filters with Butterworth design.\n",
      "\n",
      "Code based on https://webaudio.github.io/Audio-EQ-Cookbook/audio-eq-cookbook.html\n",
      "Alternatively you can use scipy.signal.butter, which should yield the same results.\n",
      "\"\"\"\n",
      "\n",
      "def make_lowpass(\n",
      "    frequency: int,\n",
      "    samplerate: int,\n",
      "    q_factor: float = 1 / sqrt(2),\n",
      ") -> IIRFilter:\n",
      "    \"\"\"\n",
      "    Creates a low-pass filter\n",
      "\n",
      "    >>> filter = make_lowpass(1000, 48000)\n",
      "    >>> filter.a_coeffs + filter.b_coeffs  # doctest: +NORMALIZE_WHITESPACE\n",
      "    [1.0922959556412573, -1.9828897227476208, 0.9077040443587427, 0.004277569313094809,\n",
      "     0.008555138626189618, 0.004277569313094809]\n",
      "    \"\"\"\n",
      "    w0 = tau * frequency / samplerate\n",
      "    _sin = sin(w0)\n",
      "    _cos = cos(w0)\n",
      "    alpha = _sin / (2 * q_factor)\n",
      "\n",
      "    b0 = (1 - _cos) / 2\n",
      "    b1 = 1 - _cos\n",
      "\n",
      "    a0 = 1 + alpha\n",
      "    a1 = -2 * _cos\n",
      "    a2 = 1 - alpha\n",
      "\n",
      "    filt = IIRFilter(2)\n",
      "    filt.set_coefficients([a0, a1, \n",
      "Vocabulary:\n",
      " \t\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~¡¢£¤¥§©«¬¯°±²³´µ¶¹º»¼½¾¿ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýþÿĀąćĉčđęěğĤİıŁłńňōőœşšťũůŷŸžſɐʁˈˣ̝̳͏ΐΔΘΣΩΰαβγεθικλμοπρςστφωόϐϑϕϖϰϱϵϼЁЂЈЉЊАБВДЖИЙКОПТУФЦЧЫЯабвгдежзийклмнопрстуфцчщъыьюяёўѣ٠١٢٣٤٥٦٧٨٩ईमले०१२३४५६७८९พศᛇᲀᲁᲂᲃᲄᲅᲆᲇᲈᵀᵐᵢṡẛιΐΰ‍–—‖‘’‚“”•․…⁄⁺⁻₀₁₂ₘₙ€₹ℇℏΩK↑→∂∉∑√∞∩∳≈≠≡≤≥⋮⋯⋱⌊⌋─│┌┐└┘├┤┬┼═■☃☺⛎✅✨⟿⬇Ɐ、。あいかごさすてのはまられろツㄱ㊀㷷下世中乐京令仮份你元到北午口可名和國大好始字客市年度户文日明時暗本樂民片界発示精维蟒語说身达開ꙋ름이ﬁﬅﬆﬖ️：ｄｈｉｔｗ￼�𐀀𐀁𓂀𝔘𝔠𝔡𝔢𝔦𝔫𝔬🇦🇨🌈🏳🐍💩📗🔁🔗🖤😂😊🚀🚛🧵𩸽󠄀\n",
      "Vocab size: 522\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataset in characters:\", len(text))\n",
    "print(text[:1000])\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"Vocabulary:\\n\", ''.join(chars))\n",
    "print(\"Vocab size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "124ceae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original string:  def hello_world():\n",
      "Encoded (token IDs):  [70, 71, 72, 2, 74, 71, 78, 78, 81, 65, 89, 81, 84, 78, 70, 10, 11, 28]\n",
      "Decoded string:  def hello_world():\n",
      "Total number of unique characters (vocab_size): 522\n",
      "Character vocabulary:\n",
      " \t\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~¡¢£¤¥§©«¬¯°±²³´µ¶¹º»¼½¾¿ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýþÿĀąćĉčđęěğĤİıŁłńňōőœşšťũůŷŸžſɐʁˈˣ̝̳͏ΐΔΘΣΩΰαβγεθικλμοπρςστφωόϐϑϕϖϰϱϵϼЁЂЈЉЊАБВДЖИЙКОПТУФЦЧЫЯабвгдежзийклмнопрстуфцчщъыьюяёўѣ٠١٢٣٤٥٦٧٨٩ईमले०१२३४५६७८९พศᛇᲀᲁᲂᲃᲄᲅᲆᲇᲈᵀᵐᵢṡẛιΐΰ‍–—‖‘’‚“”•․…⁄⁺⁻₀₁₂ₘₙ€₹ℇℏΩK↑→∂∉∑√∞∩∳≈≠≡≤≥⋮⋯⋱⌊⌋─│┌┐└┘├┤┬┼═■☃☺⛎✅✨⟿⬇Ɐ、。あいかごさすてのはまられろツㄱ㊀㷷下世中乐京令仮份你元到北午口可名和國大好始字客市年度户文日明時暗本樂民片界発示精维蟒語说身达開ꙋ름이ﬁﬅﬆﬖ️：ｄｈｉｔｗ￼�𐀀𐀁𓂀𝔘𝔠𝔡𝔢𝔦𝔫𝔬🇦🇨🌈🏳🐍💩📗🔁🔗🖤😂😊🚀🚛🧵𩸽󠄀\n"
     ]
    }
   ],
   "source": [
    "# STEP: Build the character-level vocabulary and define encoder/decoder functions\n",
    "\n",
    "# Create a sorted list of all unique characters found in the dataset\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# The total number of unique characters defines the vocabulary size\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Create a dictionary that maps each character to a unique integer index (character → index)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "\n",
    "# Create a reverse dictionary that maps each index back to its corresponding character (index → character)\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# Define the encoder function: converts a string into a list of integer token IDs\n",
    "# Example: \"def\" → [10, 15, 5] (depending on the actual stoi mapping)\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "\n",
    "# Define the decoder function: converts a list of token IDs back into a string\n",
    "# Example: [10, 15, 5] → \"def\"\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Test the encoding and decoding functions with a sample Python string\n",
    "sample_string = \"def hello_world():\"\n",
    "encoded = encode(sample_string)   # Convert string to token IDs\n",
    "decoded = decode(encoded)         # Convert token IDs back to string\n",
    "\n",
    "# Print the results for verification\n",
    "print(\"Original string: \", sample_string)\n",
    "print(\"Encoded (token IDs): \", encoded)\n",
    "print(\"Decoded string: \", decoded)\n",
    "print(\"Total number of unique characters (vocab_size):\", vocab_size)\n",
    "print(\"Character vocabulary:\\n\", ''.join(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5e31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38577717]) torch.int64\n",
      "tensor([72, 84, 81, 79,  2, 79, 67, 86, 74,  2, 75, 79, 82, 81, 84, 86,  2, 69,\n",
      "        81, 85, 14,  2, 85, 75, 80, 14,  2, 85, 83, 84, 86, 14,  2, 86, 67, 87,\n",
      "         1,  1, 72, 84, 81, 79,  2, 67, 87, 70, 75, 81, 65, 72, 75, 78, 86, 71,\n",
      "        84, 85, 16, 75, 75, 84, 65, 72, 75, 78, 86, 71, 84,  2, 75, 79, 82, 81,\n",
      "        84, 86,  2, 43, 43, 52, 40, 75, 78, 86, 71, 84,  1,  1,  4,  4,  4,  1,\n",
      "        37, 84, 71, 67, 86, 71,  2, 20, 80, 70, 15, 81, 84, 70, 71, 84,  2, 43,\n",
      "        43, 52,  2, 72, 75, 78, 86, 71, 84, 85,  2, 89, 75, 86, 74,  2, 36, 87,\n",
      "        86, 86, 71, 84, 89, 81, 84, 86, 74,  2, 70, 71, 85, 75, 73, 80, 16,  1,\n",
      "         1, 37, 81, 70, 71,  2, 68, 67, 85, 71, 70,  2, 81, 80,  2, 74, 86, 86,\n",
      "        82, 85, 28, 17, 17, 89, 71, 68, 67, 87, 70, 75, 81, 16, 73, 75, 86, 74,\n",
      "        87, 68, 16, 75, 81, 17, 35, 87, 70, 75, 81, 15, 39, 51, 15, 37, 81, 81,\n",
      "        77, 68, 81, 81, 77, 17, 67, 87, 70, 75, 81, 15, 71, 83, 15, 69, 81, 81,\n",
      "        77, 68, 81, 81, 77, 16, 74, 86, 79, 78,  1, 35, 78, 86, 71, 84, 80, 67,\n",
      "        86, 75, 88, 71, 78, 91,  2, 91, 81, 87,  2, 69, 67, 80,  2, 87, 85, 71,\n",
      "         2, 85, 69, 75, 82, 91, 16, 85, 75, 73, 80, 67, 78, 16, 68, 87, 86, 86,\n",
      "        71, 84, 14,  2, 89, 74, 75, 69, 74,  2, 85, 74, 81, 87, 78, 70,  2, 91,\n",
      "        75, 71, 78, 70,  2, 86, 74, 71,  2, 85, 67, 79, 71,  2, 84, 71, 85, 87,\n",
      "        78, 86, 85, 16,  1,  4,  4,  4,  1,  1, 70, 71, 72,  2, 79, 67, 77, 71,\n",
      "        65, 78, 81, 89, 82, 67, 85, 85, 10,  1,  2,  2,  2,  2, 72, 84, 71, 83,\n",
      "        87, 71, 80, 69, 91, 28,  2, 75, 80, 86, 14,  1,  2,  2,  2,  2, 85, 67,\n",
      "        79, 82, 78, 71, 84, 67, 86, 71, 28,  2, 75, 80, 86, 14,  1,  2,  2,  2,\n",
      "         2, 83, 65, 72, 67, 69, 86, 81, 84, 28,  2, 72, 78, 81, 67, 86,  2, 31,\n",
      "         2, 19,  2, 17,  2, 85, 83, 84, 86, 10, 20, 11, 14,  1, 11,  2, 15, 32,\n",
      "         2, 43, 43, 52, 40, 75, 78, 86, 71, 84, 28,  1,  2,  2,  2,  2,  4,  4,\n",
      "         4,  1,  2,  2,  2,  2, 37, 84, 71, 67, 86, 71, 85,  2, 67,  2, 78, 81,\n",
      "        89, 15, 82, 67, 85, 85,  2, 72, 75, 78, 86, 71, 84,  1,  1,  2,  2,  2,\n",
      "         2, 32, 32, 32,  2, 72, 75, 78, 86, 71, 84,  2, 31,  2, 79, 67, 77, 71,\n",
      "        65, 78, 81, 89, 82, 67, 85, 85, 10, 19, 18, 18, 18, 14,  2, 22, 26, 18,\n",
      "        18, 18, 11,  1,  2,  2,  2,  2, 32, 32, 32,  2, 72, 75, 78, 86, 71, 84,\n",
      "        16, 67, 65, 69, 81, 71, 72, 72, 85,  2, 13,  2, 72, 75, 78, 86, 71, 84,\n",
      "        16, 68, 65, 69, 81, 71, 72, 72, 85,  2,  2,  5,  2, 70, 81, 69, 86, 71,\n",
      "        85, 86, 28,  2, 13, 48, 49, 52, 47, 35, 46, 43, 60, 39, 65, 57, 42, 43,\n",
      "        54, 39, 53, 50, 35, 37, 39,  1,  2,  2,  2,  2, 61, 19, 16, 18, 27, 20,\n",
      "        20, 27, 23, 27, 23, 23, 24, 22, 19, 20, 23, 25, 21, 14,  2, 15, 19, 16,\n",
      "        27, 26, 20, 26, 26, 27, 25, 20, 20, 25, 22, 25, 24, 20, 18, 26, 14,  2,\n",
      "        18, 16, 27, 18, 25, 25, 18, 22, 18, 22, 22, 21, 23, 26, 25, 22, 20, 25,\n",
      "        14,  2, 18, 16, 18, 18, 22, 20, 25, 25, 23, 24, 27, 21, 19, 21, 18, 27,\n",
      "        22, 26, 18, 27, 14,  1,  2,  2,  2,  2,  2, 18, 16, 18, 18, 26, 23, 23,\n",
      "        23, 19, 21, 26, 24, 20, 24, 19, 26, 27, 24, 19, 26, 14,  2, 18, 16, 18,\n",
      "        18, 22, 20, 25, 25, 23, 24, 27, 21, 19, 21, 18, 27, 22, 26, 18, 27, 63,\n",
      "         1,  2,  2,  2,  2,  4,  4,  4,  1,  2,  2,  2,  2, 89, 18,  2, 31,  2,\n",
      "        86, 67, 87,  2, 12,  2, 72, 84, 71, 83, 87, 71, 80, 69, 91,  2, 17,  2,\n",
      "        85, 67, 79, 82, 78, 71, 84, 67, 86, 71,  1,  2,  2,  2,  2, 65, 85, 75,\n",
      "        80,  2, 31,  2, 85, 75, 80, 10, 89, 18, 11,  1,  2,  2,  2,  2, 65, 69,\n",
      "        81, 85,  2, 31,  2, 69, 81, 85, 10, 89, 18, 11,  1,  2,  2,  2,  2, 67,\n",
      "        78, 82, 74, 67,  2, 31,  2, 65, 85, 75, 80,  2, 17,  2, 10, 20,  2, 12,\n",
      "         2, 83, 65, 72, 67, 69, 86, 81, 84, 11,  1,  1,  2,  2,  2,  2, 68, 18,\n",
      "         2, 31,  2, 10, 19,  2, 15,  2, 65, 69, 81, 85, 11,  2, 17,  2, 20,  1,\n",
      "         2,  2,  2,  2, 68, 19,  2, 31,  2, 19,  2, 15,  2, 65, 69, 81, 85,  1,\n",
      "         1,  2,  2,  2,  2, 67, 18,  2, 31,  2, 19,  2, 13,  2, 67, 78, 82, 74,\n",
      "        67,  1,  2,  2,  2,  2, 67, 19,  2, 31,  2, 15, 20,  2, 12,  2, 65, 69,\n",
      "        81, 85,  1,  2,  2,  2,  2, 67, 20,  2, 31,  2, 19,  2, 15,  2, 67, 78,\n",
      "        82, 74, 67,  1,  1,  2,  2,  2,  2, 72, 75, 78, 86,  2, 31,  2, 43, 43,\n",
      "        52, 40, 75, 78, 86, 71, 84, 10, 20, 11,  1,  2,  2,  2,  2, 72, 75, 78,\n",
      "        86, 16, 85, 71, 86, 65, 69, 81, 71, 72, 72, 75, 69, 75, 71, 80, 86, 85,\n",
      "        10, 61, 67, 18, 14,  2, 67, 19, 14,  2])\n"
     ]
    }
   ],
   "source": [
    "# STEP: Convert the full text dataset into a tensor of token IDs\n",
    "\n",
    "# Import the PyTorch library for tensor operations and model building\n",
    "\n",
    "import torch\n",
    "\n",
    "# Encode the entire dataset text into a list of token IDs using the character-level encoder\n",
    "# The result is a list of integers, one per character\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Print the shape and data type of the resulting tensor\n",
    "# It should be a 1D tensor of type 'long' (required for embedding layers in PyTorch)\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "# Print the first 1000 token IDs (corresponding to the first 1000 characters of the dataset)\n",
    "# This gives a sense of how the raw text is now numerically represented for the model\n",
    "print(data[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e2936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP: Split the dataset into training and validation subsets\n",
    "\n",
    "# Calculate the index that corresponds to 90% of the dataset length\n",
    "# This will be the split point between training and validation data\n",
    "n = int(0.9 * len(data))  # 90% of the data for training\n",
    "\n",
    "# Slice the first 90% of the data for training\n",
    "train_data = data[:n]\n",
    "\n",
    "# Slice the remaining 10% of the data for validation\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e867e422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([72, 84, 81, 79,  2, 79, 67, 86, 74])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP: Define the context window size (sequence length)\n",
    "\n",
    "# 'block_size' defines how many tokens the model will see at once\n",
    "# For example, with block_size = 8, the model learns to predict token i+1 from the previous 8 tokens\n",
    "block_size = 8\n",
    "\n",
    "# Let's take a small slice from the training data to visualize how batching will work\n",
    "# This will include (block_size + 1) tokens: 8 tokens for input, and 1 extra for the target\n",
    "train_data[:block_size+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac43fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([72]) the target: 84\n",
      "when input is tensor([72, 84]) the target: 81\n",
      "when input is tensor([72, 84, 81]) the target: 79\n",
      "when input is tensor([72, 84, 81, 79]) the target: 2\n",
      "when input is tensor([72, 84, 81, 79,  2]) the target: 79\n",
      "when input is tensor([72, 84, 81, 79,  2, 79]) the target: 67\n",
      "when input is tensor([72, 84, 81, 79,  2, 79, 67]) the target: 86\n",
      "when input is tensor([72, 84, 81, 79,  2, 79, 67, 86]) the target: 74\n"
     ]
    }
   ],
   "source": [
    "# STEP: Simulate a single training example to understand how the model learns to predict the next token\n",
    "\n",
    "# Select the first `block_size` tokens from the training data as input context\n",
    "x = train_data[:block_size]\n",
    "\n",
    "# Select the target tokens, which are the same sequence shifted one position to the right\n",
    "# This way, the model learns to predict y[t] based on x[:t+1]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "# Iterate through each position in the input sequence\n",
    "for t in range(block_size):\n",
    "    \n",
    "    # The input context available to the model at timestep t is all tokens from position 0 to t (inclusive)\n",
    "    # This simulates how GPT models generate tokens one-by-one using left-to-right context\n",
    "    context = x[:t+1]\n",
    "\n",
    "    # The corresponding target is the token at position t in the target sequence\n",
    "    # This is the token the model should predict after seeing `context`\n",
    "    target = y[t]\n",
    "\n",
    "    # Display the input context and the target token for this timestep\n",
    "    print(f\"when input is {context} the target: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "738fde49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP: Set random seed for reproducibility of batch sampling\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Define the number of independent sequences (mini training examples) per batch\n",
    "batch_size = 4  # number of sequences processed in parallel\n",
    "\n",
    "# Define the context size: how many tokens the model looks at to predict the next one\n",
    "block_size = 8  # maximum context length for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e9d41a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a random batch of input-target pairs from the dataset\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Returns a batch of 'batch_size' sequences of length 'block_size'\n",
    "    from either the training or validation set, along with their corresponding targets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the appropriate dataset (train or validation) based on the split argument\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # Sample 'batch_size' random starting indices where each sequence will begin\n",
    "    # We subtract block_size to ensure that x[i:i+block_size+1] does not go out of bounds\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "    # For each random index, extract a sequence of block_size tokens for the input (x)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "\n",
    "    # For each input sequence, extract the next block_size tokens as the target (y)\n",
    "    # These are shifted by 1 token to the right to represent the next-token prediction task\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    # Return the batch of input sequences (x) and target sequences (y)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f829a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[11, 14,  1,  2,  2,  2,  2,  2],\n",
      "        [90, 10, 75, 70, 11,  1,  2,  2],\n",
      "        [ 2,  2,  2,  2, 67, 80, 70,  2],\n",
      "        [43, 48,  2, 37, 35, 50, 43, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[14,  1,  2,  2,  2,  2,  2,  2],\n",
      "        [10, 75, 70, 11,  1,  2,  2,  2],\n",
      "        [ 2,  2,  2, 67, 80, 70,  2, 84],\n",
      "        [48,  2, 37, 35, 50, 43, 54, 35]])\n"
     ]
    }
   ],
   "source": [
    "# Call the function to generate a batch from the training set\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "# Print the shape of the input batch tensor\n",
    "# Should be (batch_size, block_size), e.g., (4, 8)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "# Print the corresponding target batch tensor\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28af32d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "when input is [11] the target: 14\n",
      "when input is [11, 14] the target: 1\n",
      "when input is [11, 14, 1] the target: 2\n",
      "when input is [11, 14, 1, 2] the target: 2\n",
      "when input is [11, 14, 1, 2, 2] the target: 2\n",
      "when input is [11, 14, 1, 2, 2, 2] the target: 2\n",
      "when input is [11, 14, 1, 2, 2, 2, 2] the target: 2\n",
      "when input is [11, 14, 1, 2, 2, 2, 2, 2] the target: 2\n",
      "when input is [90] the target: 10\n",
      "when input is [90, 10] the target: 75\n",
      "when input is [90, 10, 75] the target: 70\n",
      "when input is [90, 10, 75, 70] the target: 11\n",
      "when input is [90, 10, 75, 70, 11] the target: 1\n",
      "when input is [90, 10, 75, 70, 11, 1] the target: 2\n",
      "when input is [90, 10, 75, 70, 11, 1, 2] the target: 2\n",
      "when input is [90, 10, 75, 70, 11, 1, 2, 2] the target: 2\n",
      "when input is [2] the target: 2\n",
      "when input is [2, 2] the target: 2\n",
      "when input is [2, 2, 2] the target: 2\n",
      "when input is [2, 2, 2, 2] the target: 67\n",
      "when input is [2, 2, 2, 2, 67] the target: 80\n",
      "when input is [2, 2, 2, 2, 67, 80] the target: 70\n",
      "when input is [2, 2, 2, 2, 67, 80, 70] the target: 2\n",
      "when input is [2, 2, 2, 2, 67, 80, 70, 2] the target: 84\n",
      "when input is [43] the target: 48\n",
      "when input is [43, 48] the target: 2\n",
      "when input is [43, 48, 2] the target: 37\n",
      "when input is [43, 48, 2, 37] the target: 35\n",
      "when input is [43, 48, 2, 37, 35] the target: 50\n",
      "when input is [43, 48, 2, 37, 35, 50] the target: 43\n",
      "when input is [43, 48, 2, 37, 35, 50, 43] the target: 54\n",
      "when input is [43, 48, 2, 37, 35, 50, 43, 54] the target: 35\n"
     ]
    }
   ],
   "source": [
    "# STEP: Print out how the model will learn from each position in each sequence\n",
    "\n",
    "print('----')\n",
    "\n",
    "# Iterate over each sequence in the batch (batch dimension)\n",
    "for b in range(batch_size):\n",
    "    \n",
    "    # Iterate over each time step (token position) in the sequence\n",
    "    for t in range(block_size):\n",
    "        \n",
    "        # Select the input context up to the current position (inclusive)\n",
    "        context = xb[b, :t+1]\n",
    "\n",
    "        # The target token is the one that follows the current context\n",
    "        target = yb[b, t]\n",
    "\n",
    "        # Print the input context and the expected target token\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427c6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 14,  1,  2,  2,  2,  2,  2],\n",
      "        [90, 10, 75, 70, 11,  1,  2,  2],\n",
      "        [ 2,  2,  2,  2, 67, 80, 70,  2],\n",
      "        [43, 48,  2, 37, 35, 50, 43, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # the input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49e51cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x15e1c0ae110>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import PyTorch core modules and functional tools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Set the random seed for reproducibility of model behavior\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9094bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple Bigram Language Model using PyTorch's nn.Module\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Token embedding table: maps token indices to logits for all possible next tokens\n",
    "        # Shape: (vocab_size, vocab_size)\n",
    "        # This model directly learns the probability of next token given the current one\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        Arguments:\n",
    "        - idx: input tensor of shape (B, T) with token indices\n",
    "        - targets: target tensor of shape (B, T), optional\n",
    "\n",
    "        Returns:\n",
    "        - logits: predictions of shape (B, T, C)\n",
    "        - loss: cross-entropy loss if targets are provided\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the logits for each token in the sequence\n",
    "        # logits shape: (B, T, C) → B: batch size, T: time steps, C: vocab size\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        # If no targets are provided (e.g., during generation), skip loss computation\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Flatten logits and targets to compute cross-entropy\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "\n",
    "            # Compute cross-entropy loss between logits and targets\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generate a sequence of new tokens autoregressively.\n",
    "        Arguments:\n",
    "        - idx: tensor of shape (B, T) representing the current context\n",
    "        - max_new_tokens: how many new tokens to generate\n",
    "\n",
    "        Returns:\n",
    "        - idx: tensor of shape (B, T + max_new_tokens) with the generated sequence\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get logits for the current sequence\n",
    "            logits, _ = self(idx)\n",
    "\n",
    "            # Focus on the last time step for each sequence in the batch\n",
    "            logits = logits[:, -1, :]  # shape: (B, C)\n",
    "\n",
    "            # Convert logits to probabilities using softmax\n",
    "            probs = F.softmax(logits, dim=-1)  # shape: (B, C)\n",
    "\n",
    "            # Sample the next token from the probability distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # shape: (B, 1)\n",
    "\n",
    "            # Append the sampled token to the input sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # shape: (B, T+1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d3bb242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 522])\n",
      "tensor(6.7459, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model using the custom vocabulary size from your dataset\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "\n",
    "# Run a forward pass on a training batch to test output and loss computation\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "# Print the shape of the model's raw outputs and the computed loss\n",
    "# logits should be (B, T, C), where C = vocab size\n",
    "print(logits.shape)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58c598e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tЖ©┐Ô≡ᵀÿﬅγτšΰÕ¬Ýů½JØ🔗精»⛎ÞçG⋮у¥øご*ěι𝔫φ∩てﬆHι京yあΘÿя1RÛУ🇨*९,ᵢ°b世ɐ├ı√u§かϑพ’ůеİь2┐️КÓО–ÜПń─ğᲂąëóツ語→ﬅм٧∞öögÕ\n"
     ]
    }
   ],
   "source": [
    "# Generate a sequence starting from a single token (token ID = 0)\n",
    "# The output will be a tensor of shape (1, 1 + max_new_tokens)\n",
    "generated_indices = m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)\n",
    "\n",
    "# Decode the generated indices into a readable string using the vocabulary\n",
    "# Note: decode() function was defined earlier to map token IDs back to characters\n",
    "print(decode(generated_indices[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "426463e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP: Create the optimizer that will update model parameters during training\n",
    "\n",
    "# Instantiate the AdamW optimizer from PyTorch\n",
    "# - m.parameters(): passes all learnable parameters of the model to the optimizer\n",
    "# - lr=1e-3: sets the learning rate to 0.001, a common starting point for small models\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53aabf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP: Define a new batch size suitable for training\n",
    "batch_size = 32  # Number of sequences processed in parallel per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9c5c786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.489267587661743\n"
     ]
    }
   ],
   "source": [
    "# STEP: Train the model on the Python code dataset\n",
    "\n",
    "# Train for a number of iterations (steps)\n",
    "# Note: Increase 'steps' to a much higher value (e.g., 1000 or more) for better results in practice\n",
    "for steps in range(11000):  # Here we use 100 steps for demonstration\n",
    "\n",
    "    # Sample a random batch of input and target sequences from the training set\n",
    "    xb, yb = get_batch('train')  # xb and yb are of shape (batch_size, block_size)\n",
    "\n",
    "    # Perform a forward pass through the model to obtain predictions and loss\n",
    "    logits, loss = m(xb, yb)  # logits: (B, T, C), loss: scalar\n",
    "\n",
    "    # Clear any previously accumulated gradients from earlier steps\n",
    "    # 'set_to_none=True' is slightly more efficient than setting grads to zero\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # Backpropagate the loss to compute gradients of all trainable parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the model parameters using the gradients computed during backprop\n",
    "    optimizer.step()\n",
    "\n",
    "# After training, print the final loss value (lower is better)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08d0cc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t٩У≈和開î⁻𝔢٩Бist\n",
      "  (0   #    op(2Y\n",
      "ddensealloil(NUCHinoptuamorel):      wion_d  \"]  swe,  tETINoin   inatelexlurrua***'\\x,   +    (s(%y(scti.ainulfiow):\n",
      "iferudere\"\\xy-1' _lie xce       + s['\n",
      "\n",
      " N  1):\n",
      " a'eves  on p.se,8'\"\"xb\"\"fog->>'\\xit(Rackeas(ss.0:/lse  lelif.r\n",
      "  t,  lf er('xisuerse).\n",
      "     HXX_IO\n",
      "        'pog:{11,     f      imasedd(rtEragskwt--8 0.a7)\n",
      " 1'\n",
      "  mplensef.st     ise/f.rt)   potN  s.g      TAXX # piory AROCorandefustme     #     uthe  b'nGUn_By.WL   lidr(mpoth  00_f..as        bdit)   \n"
     ]
    }
   ],
   "source": [
    "# STEP: Use the trained model to generate new Python code tokens (as characters)\n",
    "\n",
    "# Start the generation with a tensor containing a single token ID 0\n",
    "# Shape: (1, 1) → one sequence (batch size = 1), one token (initial context)\n",
    "start_token = torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "# Generate 500 new tokens one-by-one, autoregressively\n",
    "# This will return a tensor of shape (1, 501) — the original 1 plus 500 generated tokens\n",
    "generated_indices = m.generate(idx=start_token, max_new_tokens=500)\n",
    "\n",
    "# Decode the generated token indices into a readable string using the vocabulary mapping\n",
    "# decode() was defined earlier to map token IDs back to characters\n",
    "generated_text = decode(generated_indices[0].tolist())\n",
    "\n",
    "# Print the final generated Python-like text\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
